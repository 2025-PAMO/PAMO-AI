from basicmusic_generate.generator import generate_music_file
import torchaudio
import torch
import os
import numpy as np
from transformers import AutoProcessor, MusicgenForConditionalGeneration
from pydub import AudioSegment
import tempfile
import logging

logger = logging.getLogger(__name__)

processor = AutoProcessor.from_pretrained("facebook/musicgen-small")
model = MusicgenForConditionalGeneration.from_pretrained("facebook/musicgen-small")

OUTPUT_DIR = "output"
os.makedirs(OUTPUT_DIR, exist_ok=True)

async def generate_music_file(file, prompt: str) -> str:
    sr = 32000
    melody_tensor = None

    try:
        if file:
            logger.info("ğŸ”„ Step 1: í—ˆë° ì „ì²˜ë¦¬ ì‹œì‘")
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                tmp.write(await file.read())
                tmp_path = tmp.name

            from pydub import AudioSegment
            audio = AudioSegment.from_file(tmp_path)
            audio = audio.set_channels(1).set_frame_rate(sr)
            samples = np.array(audio.get_array_of_samples()).astype(np.float32) / (2 ** 15)
            melody_tensor = torch.tensor(samples, dtype=torch.float32).unsqueeze(0).contiguous()
            os.remove(tmp_path)

        logger.info("ğŸ§  Step 2: ëª¨ë¸ ì…ë ¥ êµ¬ì„± ì‹œì‘")
        if melody_tensor is not None:
            waveform = melody_tensor.squeeze().numpy()
            if waveform.ndim == 2 and waveform.shape[1] == 1:
                waveform = waveform[:, 0]
            inputs = processor(
                text=[prompt],
                audio=waveform,
                sampling_rate=sr,
                return_tensors="pt"
            )
        else:
            inputs = processor(
                text=[prompt],
                return_tensors="pt"
            )

        logger.info("ğŸ¼ Step 3: ëª¨ë¸ ì¶”ë¡  ì‹œì‘")
        output = model.generate(
            **inputs,
            do_sample=True,
            guidance_scale=1.5,
            max_new_tokens=512
        )

        output_tensor = output[0]
        if output_tensor.dim() == 1:
            output_tensor = output_tensor.unsqueeze(0)

        raw_path = os.path.join(OUTPUT_DIR, "generated_raw.wav")
        torchaudio.save(raw_path, output_tensor, sample_rate=sr)
        logger.info(f"ğŸ’¾ ì›ë³¸ ì €ì¥ ì™„ë£Œ: {raw_path}")

        # ë£¨í”„ ìƒì„±
        segment = AudioSegment.from_wav(raw_path)
        duration_ms = len(segment)
        midpoint = duration_ms // 2

        if duration_ms > 5000:
            loop_segment = segment[midpoint:]
            looped = loop_segment
            for _ in range(5):
                looped = looped.append(loop_segment, crossfade=100)
            logger.info("ğŸ” ë£¨í”„ ì²˜ë¦¬ ì™„ë£Œ")
        else:
            looped = segment
            logger.warning("âš ï¸ ë£¨í”„ ì—†ì´ ë°˜í™˜")

        final_path = os.path.join(OUTPUT_DIR, "generated_music_looped.wav")
        looped.export(final_path, format="wav")
        logger.info(f"âœ… ìµœì¢… íŒŒì¼ ì €ì¥ ì™„ë£Œ: {final_path}")

        return final_path

    except Exception as e:
        logger.exception("âŒ ìŒì•… ìƒì„± ì‹¤íŒ¨: %s", str(e))
        raise RuntimeError("ìŒì•… ìƒì„± ì‹¤íŒ¨")
